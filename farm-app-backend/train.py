import os

import joblib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils import class_weight
from xgboost import XGBClassifier

# --- CONFIGURATION (FINAL) ---
# NOTE: This path should point to the dual-label file generated by generate_data.py
DATASET_PATH = 'MOCK_DATA_100K_DUAL.csv' 

# Features MUST MATCH the legacy CSV headers exactly: n, p, k, ph, soil_moisture
FEATURES = ['n', 'p', 'k', 'ph', 'soil_moisture'] 
# Target is now the specific disease label.
TARGET_COLUMN = 'disease_label' 
CROP_COLUMN = 'crop'      


# --- 1. Load and Prepare the Data ---
try:
    df = pd.read_csv(DATASET_PATH, sep=',', skipinitialspace=True) 
    
    print(f"Data loaded successfully. Total records: {len(df)}")
except FileNotFoundError:
    print(f"Error: '{DATASET_PATH}' not found. Please check your file path.")
    exit()
except pd.errors.ParserError as e:
    print(f"\nCRITICAL ERROR: Failed to parse CSV. Detail: {e}")
    print("Action: Open the CSV file and verify that the column separator is strictly a comma.")
    exit()


# CRITICAL: Clean up column names by stripping leading/trailing whitespace
df.columns = df.columns.str.strip()
print(f"Verified columns in DataFrame: {df.columns.tolist()}")

# Define REQUIRED_COLUMNS list using the DEFINED VARIABLES AND THE SECOND TARGET
REQUIRED_COLUMNS = FEATURES + [CROP_COLUMN, 'health_status', 'disease_label'] 

# Check for missing data
df = df.dropna(subset=REQUIRED_COLUMNS)
print(f"Records after cleaning: {len(df)}")


# --- 2. Encode the Target Variable (Disease Label) ---
le = LabelEncoder()
df['target_encoded'] = le.fit_transform(df[TARGET_COLUMN])

joblib.dump(le, 'label_encoder.pkl')
print(f"Label Encoder saved to 'label_encoder.pkl'.")


# --- 2b. Initialize Scaler (CRITICAL FIX) ---
scaler = StandardScaler()
df[FEATURES] = scaler.fit_transform(df[FEATURES])

joblib.dump(scaler, 'scaler.pkl')
print(f"StandardScaler saved to 'scaler.pkl'. All numeric features are now scaled.")


# --- 2c. Create Disease to Health Status Mapping (NEW) ---
# Create a mapping dictionary {specific_disease: general_health_status}
# This is needed for the API to return both labels.
disease_to_health_map = df.set_index('disease_label')['health_status'].to_dict()

# Save the mapping
joblib.dump(disease_to_health_map, 'disease_to_health_map.pkl')
print(f"Disease to Health Status mapping saved to 'disease_to_health_map.pkl'.")


# --- 3. Training Function ---
def train_and_save_model(crop_name, dataframe, feature_cols, target_col, num_classes):
    """Filters data by crop, trains the model, and saves the trained model file."""
    
    # Filter the dataset for the current crop
    # FIX APPLIED: Reset the index of the filtered DataFrame
    crop_df = dataframe[dataframe[CROP_COLUMN] == crop_name].copy().reset_index(drop=True)
    
    if len(crop_df) < 10: 
        print(f"Skipping {crop_name}: Insufficient data ({len(crop_df)} records).")
        return

    print(f"\n--- Training Model for {crop_name} (Samples: {len(crop_df)}) ---")

    X = crop_df[feature_cols]
    y = crop_df['target_encoded'] # Use the encoded target column

    # --- CRITICAL FIX: Calculate Sample Weights ---
    sample_weights_raw = class_weight.compute_sample_weight(
        class_weight='balanced',
        y=y 
    )
    
    # Split data (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Map the calculated weights back to the training subset based on the new 0-based index
    weights_train = sample_weights_raw[X_train.index.values]

    # Initialize and train the XGBoost Classifier
    model = XGBClassifier(
        objective='multi:softmax',
        num_class=num_classes,
        use_label_encoder=False, 
        eval_metric='mlogloss',
        n_estimators=400,          
        learning_rate=0.02,        
        random_state=42
    )
    
    # --- CRITICAL CHANGE: Pass sample_weight to the fit method ---
    model.fit(X_train, y_train, sample_weight=weights_train) 

    # Evaluate the model
    score = model.score(X_test, y_test)
    
    # --- ACCURACY LOGGING ADDED BACK ---
    print(f"--- {crop_name} Model Accuracy: {score:.4f} ---")

    # Save the trained model
    model_filepath = f'models/{crop_name.lower().replace(" ", "_")}_disease_model.json'
    model.save_model(model_filepath)
    print(f"Model saved successfully to: {model_filepath}")


# --- 4. Main Training Loop ---
if __name__ == '__main__':
    
    if not os.path.exists('models'):
        os.makedirs('models')
        
    supported_crops = df[CROP_COLUMN].unique()
    num_classes = len(le.classes_)

    if len(supported_crops) == 0:
        print("No valid crops found in the dataset. Exiting.")
    else:
        print(f"Starting training for crops: {supported_crops}")
        for crop in supported_crops:
            train_and_save_model(crop, df, FEATURES, TARGET_COLUMN, num_classes)

    print("\nAll model training complete. You now have your model files ready for deployment!")
import os

import joblib
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.utils import class_weight
from xgboost import XGBClassifier

# --- CONFIGURATION (FINAL) ---
# NOTE: This path should point to the dual-label file generated by generate_data.py
DATASET_PATH = 'MOCK_DATA_100K_DUAL.csv' 

# Features MUST MATCH the legacy CSV headers exactly: n, p, k, ph, soil_moisture
FEATURES = ['n', 'p', 'k', 'ph', 'soil_moisture'] 
# Target is now the specific disease label.
TARGET_COLUMN = 'disease_label' 
CROP_COLUMN = 'crop'      


# --- 1. Load and Prepare the Data ---
try:
    df = pd.read_csv(DATASET_PATH, sep=',', skipinitialspace=True) 
    
    print(f"Data loaded successfully. Total records: {len(df)}")
except FileNotFoundError:
    print(f"Error: '{DATASET_PATH}' not found. Please check your file path.")
    exit()
except pd.errors.ParserError as e:
    print(f"\nCRITICAL ERROR: Failed to parse CSV. Detail: {e}")
    print("Action: Open the CSV file and verify that the column separator is strictly a comma.")
    exit()


# CRITICAL: Clean up column names by stripping leading/trailing whitespace
df.columns = df.columns.str.strip()
print(f"Verified columns in DataFrame: {df.columns.tolist()}")

# Define REQUIRED_COLUMNS list using the DEFINED VARIABLES AND THE SECOND TARGET
REQUIRED_COLUMNS = FEATURES + [CROP_COLUMN, 'health_status', 'disease_label'] 

# Check for missing data
df = df.dropna(subset=REQUIRED_COLUMNS)
print(f"Records after cleaning: {len(df)}")


# --- 2. Encode the Target Variable (Disease Label) ---
le = LabelEncoder()
df['target_encoded'] = le.fit_transform(df[TARGET_COLUMN])

joblib.dump(le, 'label_encoder.pkl')
print(f"Label Encoder saved to 'label_encoder.pkl'.")


# --- 2b. Initialize Scaler (CRITICAL FIX) ---
scaler = StandardScaler()
df[FEATURES] = scaler.fit_transform(df[FEATURES])

joblib.dump(scaler, 'scaler.pkl')
print(f"StandardScaler saved to 'scaler.pkl'. All numeric features are now scaled.")


# --- 2c. Create Disease to Health Status Mapping (NEW) ---
# Create a mapping dictionary {specific_disease: general_health_status}
# This is needed for the API to return both labels.
disease_to_health_map = df.set_index('disease_label')['health_status'].to_dict()

# Save the mapping
joblib.dump(disease_to_health_map, 'disease_to_health_map.pkl')
print(f"Disease to Health Status mapping saved to 'disease_to_health_map.pkl'.")


# --- 3. Training Function ---
def train_and_save_model(crop_name, dataframe, feature_cols, target_col, num_classes):
    """Filters data by crop, trains the model, and saves the trained model file."""
    
    # Filter the dataset for the current crop
    # FIX APPLIED: Reset the index of the filtered DataFrame
    crop_df = dataframe[dataframe[CROP_COLUMN] == crop_name].copy().reset_index(drop=True)
    
    if len(crop_df) < 10: 
        print(f"Skipping {crop_name}: Insufficient data ({len(crop_df)} records).")
        return

    print(f"\n--- Training Model for {crop_name} (Samples: {len(crop_df)}) ---")

    X = crop_df[feature_cols]
    y = crop_df['target_encoded'] # Use the encoded target column

    # --- CRITICAL FIX: Calculate Sample Weights ---
    sample_weights_raw = class_weight.compute_sample_weight(
        class_weight='balanced',
        y=y 
    )
    
    # Split data (80% training, 20% testing)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)
    
    # Map the calculated weights back to the training subset based on the new 0-based index
    weights_train = sample_weights_raw[X_train.index.values]

    # Initialize and train the XGBoost Classifier
    model = XGBClassifier(
        objective='multi:softmax',
        num_class=num_classes,
        use_label_encoder=False, 
        eval_metric='mlogloss',
        n_estimators=400,          
        learning_rate=0.02,        
        random_state=42
    )
    
    # --- CRITICAL CHANGE: Pass sample_weight to the fit method ---
    model.fit(X_train, y_train, sample_weight=weights_train) 

    # Evaluate the model
    score = model.score(X_test, y_test)
    
    # --- ACCURACY LOGGING ADDED BACK ---
    print(f"--- {crop_name} Model Accuracy: {score:.4f} ---")

    # Save the trained model
    model_filepath = f'models/{crop_name.lower().replace(" ", "_")}_disease_model.json'
    model.save_model(model_filepath)
    print(f"Model saved successfully to: {model_filepath}")


# --- 4. Main Training Loop ---
if __name__ == '__main__':
    
    if not os.path.exists('models'):
        os.makedirs('models')
        
    supported_crops = df[CROP_COLUMN].unique()
    num_classes = len(le.classes_)

    if len(supported_crops) == 0:
        print("No valid crops found in the dataset. Exiting.")
    else:
        print(f"Starting training for crops: {supported_crops}")
        for crop in supported_crops:
            train_and_save_model(crop, df, FEATURES, TARGET_COLUMN, num_classes)

    print("\nAll model training complete. You now have your model files ready for deployment!")

    
